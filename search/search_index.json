{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83c\udfe0 Introduction","text":"Welcome to the Beacon2 CBI Tools Documentation ! beacon2-cbi-tools"},{"location":"#overview","title":"Overview","text":"<p>beacon2-cbi-tools is a suite of tools originally developed as part of the ELIXIR\u2013Beacon v2 Reference Implementation, now continuing under CNAG Biomedical Informatics. It provides essential functionality around the Beacon Friendly Format (BFF) data exchange format, including:</p> <ul> <li>Validating XLSX/JSON files against Beacon v2 schemas</li> <li>Converting VCF files into BFF (genomicVariations)</li> <li>Loading BFF data (metadata and genomic variations) into MongoDB</li> </ul> <p>This toolkit streamlines data preparation, validation, and ingestion for federated genomic and phenotypic data sharing under Beacon v2. The resulting BFF-formatted data can be used with any implementation of the Beacon v2 API specification that operates on MongoDB.</p>"},{"location":"#tools-included","title":"Tools Included","text":""},{"location":"#bff-tools-script-binbff-tools","title":"BFF-Tools script (<code>bin/bff-tools</code>):","text":"<p>A command-line tool for converting VCF data into BFF format and inserting the resulting BFF data into a MongoDB instance.</p> <p>The tool offers four modes:</p> <ol> <li> <p>vcf: Convert a VCF.gz file into BFF format.</p> </li> <li> <p>load: Load BFF-formatted data into a MongoDB instance.</p> </li> <li> <p>full: Perform both VCF conversion and MongoDB loading.</p> </li> <li> <p>validate: Validate XLSX or JSON metadata against Beacon v2 schemas and serialize into BFF. An Excel template is provided to help structure your metadata.</p> </li> </ol>"},{"location":"#utility-suite","title":"Utility Suite:","text":"<p>A collection of support tools to aid in data ingestion. Key among them:</p> <ul> <li> <p>BFF-Browser:  </p> <p>A web application for interactive visualization of BFF data, particularly <code>genomicVariations</code> and <code>individuals</code>.</p> </li> <li> <p>BFF-Portal:  </p> <p>A simple API and web application to query BFF data via MongoDB.</p> </li> </ul>"},{"location":"#cineca-synthetic-cohort-europe_uk1","title":"CINECA Synthetic Cohort - EUROPE_UK1:","text":"<p>A synthetic dataset for testing and demonstration purposes.</p>"},{"location":"#system-diagram","title":"System Diagram","text":"<pre><code>            * Beacon v2 - CBI Tools *\n\n                ___________\n          XLSX  |          |\n           or   | Metadata | (incl. Phenotypic data)\n          JSON  |__________|\n                     |\n                     |\n                     | bff-tools validate\n                     |                                   Beacon v2\n_________        ____v____            __________          ______\n|       |        |       |            |          |        |     | &lt;---- Request\n|  VCF  | -----&gt; |  BFF  | ---------&gt; | Database | &lt;----&gt; | API |\n|_______|        |_ _____|            |__________|        |_____| ----&gt; Response\n                     |                  MongoDB\n       bff-tools vcf |  bff-tools load\n                     |\n                     |\n                  Optional (utils)\n                     |\n                _____v_____\n                |         |\n                | utils/  |\n                |  bff-   |\n                | browser | Visualization\n                | (beta)  |\n                |_________|\n\n-----------------------------------------------|||---------------------------\nbeacon2-cbi-tools                                     e.g. beacon2-ri-api\n                                                           beacon2-pi-api\n                                                           java-beacon-v2.api   \n                                                           ...\n</code></pre>"},{"location":"data-beaconization/","title":"Tutorial (Data \"beaconization\")","text":"<p>In this tutorial it is expected to already have the Data ingestion tools downloaded, installed and set up following the instructions in Installation.</p> <p>Let's start by the simplest case. Imagine that you have:</p> <ul> <li>Metadata (including phenotypic data) in your system, labelled according to your internal nomenclature.</li> <li>A VCF file.</li> </ul>"},{"location":"data-beaconization/#previous-steps","title":"Previous steps","text":""},{"location":"data-beaconization/#connect-to-the-container","title":"Connect to the container","text":"<p>The beacon2-cbi-tools container is running in detached mode or in the background. To connect, you should invoke from your terminal:</p> <p><pre><code>docker exec -ti beacon2-cbi-tools bash\n</code></pre> Now you should be inside the <code>beacon2-cbi-tools</code> container to start the Data beaconization.</p>"},{"location":"data-beaconization/#step-1","title":"STEP 1","text":"<p>First, we are going to convert your metadata (sequencing methodology, bioinformatics tools, phenotypic data, etc.) to the format of the Beacon v2 Models. As input, we will be using this XLSX template.</p> <p>About the XLSX template</p> <p>The XLSX template consists of seven sheets that match the Beacon v2 Models. The template has the purpose of facilitating users' transformation of the data (likely in tabular form) to the hierarchical structure that we have in the Beacon v2 Models. The header nomenclature gives a hint about if the data will be later stored as an <code>object</code> (naming contains <code>.</code>) or as an <code>array</code> (naming contains <code>_</code>). Each column has its own format (e.g., string, date, CURIE). These formats can browsed in the documentation.  We recommend using the provided XLSX for the synthetic data as a reference.</p> <p>The first thing you need to do is manually map/convert your metadata to match the syntax of the provided XLSX file, ensuring you fill it out accurately.</p> <p></p> <p>Note</p> <p>Normally, people don't fill out the sheet (tab) named <code>genomicVariations</code> as this info will be taken from the annotated VCF (see STEP 2).</p> <p>Once you have completed filling out the Excel file, you can proceed to validate it. At this stage, it\u2019s common to have uncertainties about your mapping to Beacon v2 syntax. Luckily, the B2RI utility <code>bff-validator</code> is there to assist you with this task. The validator ensures that all values in the XLSX file conform to the specifications outlined in the Beacon v2 Models default schemas. Technically, this process is referred to as validating data against JSON Schemas.</p> <pre><code>bin/bff-tools validate -i your_xlsx_file.xlsx --out-dir your_bff_dir\n</code></pre> <p>When you run it, it's highly probable that you'll encounter errors or warnings related to your data. The <code>bff-tools validate</code> tool is designed to detect these errors and provide explanations for the validation failures. Please ensure to address all the issues within the XLSX file. Feel free to run the script as many times as needed. :-)</p> Example Errors <p>Example 1:</p> <pre><code>Row 1:\n/ethnicity/id: String does not match ^\\w[^:]+:.+$.\n</code></pre> <p>This error indicates that the <code>id</code> within <code>ethnicity</code> does not conform to the CURIE convention. The expected format is a string that matches the specified regular expression.</p> <p>Example 2:</p> <p><pre><code>Row 1:\n/id: Missing property.\n</code></pre> This error suggests that the <code>id</code> attribute is missing for the individual in question.</p> <p>Example 3: (Non-error)</p> <p>As the specification is currently under development, warnings may appear for fields that appear correct. An example is the <code>oneOf</code> error, as shown below:</p> <pre><code>Row 1:\n/diseases/0/ageOfOnset: oneOf rules 0, 1 match.\n</code></pre> <p>About Unicode</p> <p>Unicode characters are permitted as values in cells. However, be cautious when copying and pasting from external sources, as this can introduce unexpected characters in unintended places. If the <code>bff-validator</code> generates errors that are difficult to diagnose, consider using the <code>--ignore-validation</code> flag to proceed and then examine the generated JSON files for errors. After identifying and correcting the errors in the original Excel file, rerun the validation process without the <code>--ignore-validation</code> flag. For more detailed information, please visit this page.</p> <p>At some point, you won't encounter any validation errors. By then, the script should have generated 6 text files, which we refer to as the Beacon Friendly Format. These files are in JSON format (specifically, JSON arrays) and will be utilized later in STEP 3.</p> <p></p> <p>Congratulations! Now you can go to STEP 2.</p>"},{"location":"data-beaconization/#step-2","title":"STEP 2","text":"<p>Now that you have processed the metadata, it's time to process the VCF file.</p> <p>About VCF types</p> <p>Currently, the B2RI only handles VCFs coming from DNAseq experiments (e.g., WES, WGS, gene panels, etc.). The VCFs can be either single or multisample.  As of this writing, structural variants in VCF files are not being parsed. (There is a Scout working group currently developing Beacon v2 specifications for structural variants.) We hope to implement this feature in future versions. Copy Number Variations are not supported. The <code>variation</code> property uses the <code>LegacyVariation</code> sub-property.</p> <p>The VCF file has to be gzipped (or bgzipped). What we are going to do it's to annotate it (or re-annotate it if your file already has annotations) with SnpEff and SnpSift and transform the format so that it becames the 7th BFF file (i.e., <code>genomicVariationsVcf.json</code>). </p> <pre><code>bin/bff-tools vcf   -t 4       -i input.vcf.gz -p param_file.yaml\n     |         |      |         |              |\n    exe      mode   #threads   &lt;vcf&gt;           parameters file (optional)\n</code></pre> <p>Here we are using <code>bff-tools</code> script in mode vcf. This mode is one of the four available [vcf|load|full|validate]. </p> <p>The parameters file is optional if you want to use the default value (hg19) but it is needed if you want to change them. Note that you must provide the reference genome (unless you're using <code>hg19</code> which is the default one) that was used to create your VCF. See all the script options here.</p> <p>The <code>param_file</code> (<code>YAML</code>) should look something like this:</p> <pre><code>---\ngenome: hs37\n</code></pre> <p>If you want to create an <code>HTML</code> file to be later viewed with the BFF Browser utility your <code>YAML</code> file should look like this:</p> <pre><code>---\ngenome: hs37\nbff2html: true\n</code></pre> <p>Note about timing</p> <p>We made the script as fast as we possibly could with a scripting language. In this regard, the processing time scales linearly with the #variants, but it's also affected by the #samples. For instance, 1M variants with 2,500 samples will take around ~20-25 min.</p> <p>If something is wrong with the input files, the script will complain and provide possible solutions.</p> <p></p> <p>Once completed, you will end up with a dir like this one <code>beacon_XXXXXXX/vcf</code>. Inside, you will find <code>genomicVariationsVcf.json.gz</code>, the 7th BFF file.</p> <p>About disk usage</p> <p>During the annotation process, multiple intermediate VCF files are created (and kept). They're all compressed, but still they will be as big as your original VCF. On top of that, <code>genomicVariationsVcf.json.gz</code> file can be huge. In summary, please allocate up to 10x times the space of your original VCF. Feel free to erase the temporary VCF files <code>beacon_XXXXXXX/vcf/*vcf.gz</code> once the job is completed.</p> <p>Now that you have the 7 JSON files it's time to go to the STEP 3.</p>"},{"location":"data-beaconization/#step-3","title":"STEP 3","text":"<p>The objective of this step is to load (a.k.a. ingest) the 7 JSON files into MongoDB. Once loaded in MongoDB, they are named collections.</p> <p>For doing this we will use again <code>bff-tools</code> script, but this time in mode <code>load</code>.</p> <p>Let's assume that we have the 6 files from STEP 1 in the directory <code>my_bff_dir</code> and the file from STEP 2 at <code>beacon_XXXXXXX</code>. </p> <p>We will add these values to a new parameters (YAML) file:</p> <pre><code>---\nbff:\n  metadatadir: my_bff_dir\n  # You can change the name of the JSON files\n  runs: runs.json\n  cohorts: cohorts.json\n  biosamples: biosamples.json\n  individuals: individuals.json\n  analyses: analyses.json\n  datasets: datasets.json\n  # Note that genomicVariationsVcf is not affected by &lt;metadatadir&gt;\n  genomicVariationsVcf: beacon_XXXXXXX/vcf/genomicVariationsVcf.json.gz\n</code></pre> <p>Finally, you execute this command</p> <pre><code>bin/bff-tools load -p param_file.yaml\n       |       |        |\n      exe     mode     paramaters file\n</code></pre> <p>If everything goes well, all your data should be loaded into an instance of MongoDB.</p> <p>Note</p> <p>To exit this container you just need to type \"exit\".</p> <p>Using <code>mongoimport</code> for data ingestion</p> <p>As mentioned in STEP 3, the <code>bff-tools</code> script in <code>load</code> mode is responsible for loading the data (ingestion and indexing) in MongoDB. You can view the detailed indexing process executed by <code>beacon2-cbi-tools</code> here. The system utilizes both single field and text indices. Notably, when new data is introduced to MongoDB, the existing indexes are automatically updated to include this new data.</p> <p>If you choose to handle data ingestion personally using a CLI tool, the <code>beacon2-cbi-tools</code> container provides the MongoDB utility <code>mongoimport</code> for this purpose. Below is an example of how to execute it:</p> <pre><code>mongoimport --jsonArray --uri \"mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin\" --file analyses.json --collection analyses \nmongoimport --jsonArray --uri \"mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin\" --file biosamples.json --collection biosamples \nmongoimport --jsonArray --uri \"mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin\" --file cohorts.json --collection cohorts \nmongoimport --jsonArray --uri \"mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin\" --file datasets.json --collection datasets \nmongoimport --jsonArray --uri \"mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin\" --file individuals.json --collection individuals \nmongoimport --jsonArray --uri \"mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin\" --file runs.json --collection runs \nmongoimport --jsonArray --uri \"mongodb://root:example@127.0.0.1:27017/beacon?authSource=admin\" --file genomicVariationsVcf.json --collection genomicVariations   \n</code></pre> <p>Again, remember that if you follow this alternative path, you will have to index your MongoDB data. Indexes can affect the performance of your Beacon v2 API.</p> <p>Note about MongoDB</p> <p>As with any other database, it is possible to perform queries directly to MongoDB. In our case, the database is named beacon and contains the ingested collections. For doing so, you will need to use one of the many UI (we have included Mongo Express), the <code>mongosh</code> (also included) or use any of the MongoDB drivers that exist for most programming languages. As an example, we have included an utility <code>bff-portal</code> that enables you to make simple queries (see instructions here). For a more comprehensive description check MongoDB literature.</p>"},{"location":"quick-start/","title":"Quick Start <code>bff-tools</code>","text":"<p>This guide provides a one-page cheat sheet for common commands:</p> <pre><code># Display help for the tool\nbin/bff-tools --help\n</code></pre> <pre><code># Display man for the tool\nbin/bff-tools --man\n</code></pre> <pre><code># Convert VCF to BFF\nbin/bff-tools vcf -i test/test_1000G.vcf.gz -p test/param.yaml\n</code></pre> <pre><code># Validate metadata and convert to BFF\nmkdir bff_out\nbin/bff-tools validate -i Beacon-v2-Models_CINECA_UK1.xlsx --out-dir bff_out\n</code></pre>"},{"location":"quick-start/#usage","title":"Usage","text":"Full UsageNotes on <code>bff-tool validate</code> <p># NAME</p> <p><code>bff-tools</code>: A unified command-line toolkit for working with Beacon v2 Models data. It allows users to annotate and convert VCF files into the <code>genomicVariations</code> entity using the Beacon-Friendly Format (BFF), validate metadata files (XLSX or JSON) against Beacon v2 schema definitions and load BFF-formatted data into a MongoDB instance.</p> <p>This tool is part of the <code>beacon2-cbi-tools</code> repository and is designed to support Beacon v2 data ingestion pipelines, metadata validation workflows, and federated data sharing initiatives.</p> <p># SYNOPSIS</p> <p>bff-tools &lt;mode&gt; [-arguments] [-options]</p> <pre><code> Mode:\n   vcf\n      -i | --input &lt;file&gt;            Requires a VCF.gz file\n                                     (May also use a parameters file)\n\n   load\n                                     (Requires a parameters file specifying BFF files)\n\n    full (vcf + load)\n      -i | --input &lt;file&gt;            Requires a VCF.gz file\n                                     (May also use a parameters file)\n\n      Options (for vcf / load / full):\n      -c | --config &lt;file&gt;           Requires a configuration file\n      -p | --param &lt;file&gt;            Requires a parameters file (optional)\n      -projectdir-override &lt;path&gt;    Custom project directory path (overrides config)\n      -t | --threads &lt;number&gt;        Number of threads (optional, mainly for VCF)\n\n   validate\n      -i | --input &lt;file(s)&gt;         One or more XLSX/JSON metadata files\n      -s | --schema-dir &lt;directory&gt;  Directory containing JSON schemas\n      -o | --out-dir &lt;directory&gt;     Output directory for validated data\n      -gv                            Set this option if you want to process &lt;genomicVariations&gt; entity\n      -ignore-validation             Writes JSON collection regardless of results from validation against JSON schemas (AYOR!)\n\n      Experimental:\n      -gv-vcf                        Set this option to read &lt;genomicVariations.json&gt; from &lt;beacon vcf&gt; (with one document per line)\n\n    Generic Options:\n      -h                             Brief help message\n      -man                           Full documentation\n      -v                             Display version information\n      -debug &lt;level&gt;                 Print debugging information (1 to 5)\n      -verbose                       Enable verbosity\n      -nc | --no-color               Do not print colors to STDOUT\n</code></pre> <p># DESCRIPTION</p> <p>### <code>bff-tools</code></p> <p><code>bff-tools</code> is a command-line toolkit with four operational modes for working with Beacon v2 data:</p> <p># HOW TO RUN <code>bff-tools</code></p> <p>This script supports four modes: <code>vcf</code>, <code>load</code>, <code>full</code>, and <code>validate</code>.</p> <p>* Mode <code>vcf</code></p> <p>Annotates a gzipped VCF file (optional) and serializes it into the Beacon-Friendly Format (BFF) as <code>genomicVariationsVcf.json.gz</code>.</p> <p>* Mode <code>load</code></p> <p>Loads BFF-formatted JSON files - including metadata and genomic variations - into a MongoDB instance.</p> <p>* Mode <code>full</code></p> <p>Combines <code>vcf</code> and <code>load</code>: it processes a VCF file and ingests the resulting data into MongoDB.</p> <p>* Mode <code>validate</code></p> <p>Validates metadata files (XLSX or JSON) against the Beacon v2 schema definitions and serializes them into BFF JSON collections.  Note: This mode uses a separate internal script and does not require a parameters or configuration file.</p> <p>To perform these tasks, you may need:</p> <ul> <li> <p>A gzipped VCF file (for modes: <code>vcf</code> and <code>full</code>)</p> <p>Note: It does not need to be bgzipped.</p> </li> <li> <p>A parameters file (optional)</p> <p>YAML file with job-specific values and metadata file references. Recommended for structured processing.</p> </li> <li> <p>BFF JSON files (required for modes: <code>load</code> and <code>full</code>)</p> <p>See Beacon-Friendly Format (BFF) for a detailed explanation.</p> </li> <li> <p>Metadata files (XLSX or JSON) (for mode: <code>validate</code>)</p> <p>You can start with the provided Excel template and use <code>--gv</code> or <code>--ignore-validation</code> flags if needed.</p> </li> <li> <p>Threads (only for <code>vcf</code> and <code>full</code> modes)</p> <p>You can set the number of threads using <code>-t</code>. However, since SnpEff doesn't parallelize efficiently, it's best to use <code>-t 1</code> and distribute the work (e.g., by chromosome) using GNU <code>parallel</code> or the included queue system).</p> </li> </ul> <p><code>bff-tools</code> will create an independent project directory <code>projectdir</code> and store all needed information needed there. Thus, many concurrent calculations are supported.  Note that <code>bff-tools</code> will treat your data as read-only (i.e., will not modify your original files).</p> <p>Annex: Parameters file (YAML)</p> <p>Example for <code>vcf</code> mode:</p> <pre><code> --\n genome: hs37 # default hg19\n annotate: true # default true\n bff2html: true # default false\n</code></pre> <p>Example for <code>load</code> mode:</p> <pre><code> --\n bff:\n   metadatadir: .\n   analyses: analyses.json\n   biosamples: biosamples.json\n   cohorts: cohorts.json\n   datasets: datasets.json\n   individuals: individuals.json\n   runs: runs.json\n   # Note that genomicVariationsVcf is not affected by &lt;metadatadir&gt;\n   genomicVariationsVcf: beacon_XXXX/vcf/genomicVariationsVcf.json.gz\n projectdir: my_project\n</code></pre> <p>Example for <code>full</code> mode:</p> <pre><code> --\n genome: hs37 # default hg19\n annotate: true # default true\n bff:\n   metadatadir: .\n   analyses: analyses.json\n   runs: runs.json\n projectdir: my_project\n</code></pre> <p>Please find below a detailed description of all parameters (alphabetical order):</p> <ul> <li> <p>annotate</p> <p>When the annotate parameters is set to <code>true</code> (default), the tool will perform annotation on the provided VCF file. This process involves running snpEff to enrich the VCF with annotation data by leveraging databases such as dbNFSP, ClinVar, and COSMIC. In this mode, the tool will generate and populate the ANN fields based on the analysis.</p> <p>If the annotate parameters is iset to <code>false</code>, the tool assumes that the VCF file has already been annotated (i.e., it already contains the ANN fields). In this case, it will skip the annotation step and directly parse the existing ANN fields. If you choose this route, please make sure to modify the file <code>lib/internal/complete/config.yaml</code> with your own values.</p> </li> <li> <p>bff</p> <p>Location for the Beacon Friendly Format JSON files.</p> </li> <li> <p>center</p> <p>Experimental feature. Not used for now.</p> </li> <li> <p>datasetid</p> <p>An unique identifier for the dataset present in the input VCF. Default value is 'id_1'</p> </li> <li> <p>genome</p> <p>Your reference genome.</p> <p>Accepted values: hg19, hg38 and hs37.</p> <p>If you used GATKs GRCh37/b37 set it to hg19.</p> <p>Not supported: NCBI36/hg18, NCBI35/hg17, NCBI34/hg16, hg15 and older.</p> </li> <li> <p>organism</p> <p>Experimental feature. Not used for now.</p> </li> <li> <p>bff2html</p> <p>Set bff2html to <code>true</code> to create HTML for the BFF Genomic Variations Browser.</p> </li> <li> <p>projectdir</p> <p>The prefix for dir name (e.g., 'cancer_sample_001'). Note that it can also contain a path (e.g., /workdir/cancer_sample_001).  The script will automatically add an unique identifier to each job.</p> </li> <li> <p>technology</p> <p>Experimental feature. Not used for now.</p> </li> </ul> <p>Examples:</p> <pre><code> $ bin/bff-tools vcf -i input.vcf.gz\n\n $ bin/bff-tools vcf -i input.vcf.gz -p param.yaml -projectdir-override beacon_exome_id_123456789\n\n $ bin/bff-tools load -p param_file  # MongoDB load only\n\n $ bin/bff-tools full -t 1 --i input.vcf.gz -p param_file  &gt; log 2&gt;&amp;1\n\n $ bin/bff-tools full -t 1 --i input.vcf.gz -p param_file -c config_file &gt; log 2&gt;&amp;1\n\n $ bin/bff-tools validate -i my_data.xlsx -o outdir\n\n $ nohup $path_to_beacon/bin/bff-tools full -i input.vcf.gz -verbose\n\n $ parallel \"bin/bff-tools vcf -t 1 -i chr{}.vcf.gz  &gt; chr{}.log 2&gt;&amp;1\" ::: {1..22} X Y\n</code></pre> <p>NB: If you don't want colors in the output use the flag <code>--no-color</code>. If you did not use the flag and want to get rid off the colors in your printed log file use this command to parse ANSI colors:</p> <pre><code> perl -pe 's/\\x1b\\[[0-9;]*[mG]//g'\n</code></pre> <p>Note: The script creates <code>log</code> files for all the processes. For instance, when running in <code>vcf</code> mode you can check via <code>tail -f</code> command:</p> <pre><code> $ tail -f &lt;your_job_id/vcf/run_vcf2bff.log\n</code></pre> <p>## WHAT IS THE BEACON FRIENDLY FORMAT (BFF)</p> <p>The Beacon Friendly Format is a data exchange format consisting up to  7 JSON files (JSON arrays) that match the 7 schemas from Beacon v2 Models.</p> <p>Six files correspond to Metadata (<code>analyses.json,biosamples.json,cohorts.json,datasets.json,individuals.json,runs.json</code>) and one corresponds to variations (<code>genomicVariations.json</code>).</p> <p>Normally, <code>bff-tools</code> script is used to create <code>genomicVariations</code> JSON file. The other 6 files are created with this utility (part of the distribution). See instructions here.</p> <p>Once we have all seven files, then we can proceed to load the data into MongoDB.</p> <p># COMMON ERRORS: SYMPTOMS AND TREATMENT</p> <pre><code> * Perl: \n         * Execution errors:\n           - Error with YAML::XS\n             Solution: Make sure the YAML (config.yaml or parameters file) is well formatted (e.g., space after param:' ').\n\n * Bash: \n         (Possible errors that can happen when the embeded Bash scripts are executed)\n         * bcftools errors: bcftools is nit-picky about VCF fields and nomenclature of contigs/chromosomes in reference genome\n                =&gt; Failed to execute: beacon_161855926405757/run_vcf2bff.sh\n                   Please check this file beacon_161855926405757/run_vcf2bff.log\n           - Error: \n                  # Running bcftools\n                  [E::faidx_adjust_position] The sequence \"22\" was not found\n             Solution: Make sure you have set the correct genome (e.g., hg19, hg38 or hs37) in parameters_file.\n                       In this case bcftools was expecting to find 22 in the &lt;*.fa.gz&gt; file from reference genome, but found 'chr22' instead.\n                 Tips:\n                      - hg{19,38} use 'chr' in chromosome naming (e.g., chr1)\n                      - hs37 does not use 'chr' in chromosome naming (e.g., 1)\n\n            - Error\n                 # Running bcftools\n                 INFO field IDREP only contains 1 field, expecting 2\n              Solution: Please Fix VCF info field manually (or get rid of problematic fields with bcftools)\n                        e.g., bcftools annotate -x INFO/IDREP input.vcf.gz | gzip &gt; output.vcf.gz\n                              bcftools annotate -x INFO/MLEAC,INFO/MLEAF,FMT/AD,FMT/PL input.vcf.gz  | gzip &gt; output.vcf.gz\n\n   NB: The bash scripts can be executed \"manually\" in the beacon_XXX dir. You must provide the \n       input vcf as an argument. This is a good option for debugging.\n</code></pre> <p>## KNOWN ISSUES</p> <pre><code> * Some Linux distributions do not include perldoc and thus Perl's library Pod::Usage will complain.\n   Please, install it (sudo apt install perl-doc) if needed.\n</code></pre> <p># CITATION</p> <p>The author requests that any published work that utilizes B2RI includes a cite to the the following reference:</p> <p>Rueda, M, Ariosa R. \"Beacon v2 Reference Implementation: a toolkit to enable federated sharing of genomic and phenotypic data\". Bioinformatics, btac568, doi.org/10.1093/bioinformatics/btac568</p> <p># AUTHOR</p> <p>Written by Manuel Rueda, PhD. Info about CNAG can be found at https://www.cnag.eu</p> <p>Credits: </p> <pre><code> * Sabela De La Torre (SDLT) created a Bash script for Beacon v1 to parse vcf files L&lt;https://github.com/ga4gh-beacon/beacon-elixir&gt;.\n * Toshiaki Katayamai re-implemented the Beacon v1 script in Ruby.\n * Later Dietmar Fernandez-Orth (DFO) modified the Ruby for Beacon v2 L&lt;https://github.com/ktym/vcftobeacon and added post-processing with R, from which I borrowed ideas to implement vcf2bff.pl.\n * DFO for usability suggestions and for creating bcftools/snpEff commands.\n * Roberto Ariosa for help with MongoDB implementation.\n * Mauricio Moldes helped with the containerization.\n</code></pre> <p># COPYRIGHT and LICENSE</p> <p>This PERL file is copyrighted. See the LICENSE file included in this distribution.</p> <p>For executing <code>bff-validator</code> you will need:</p> <ul> <li> <p>Input file:</p> <p>You have two options:</p> <p>A) A XLSX file consisting of multiple sheets. A template version of this file is provided with this installation.</p> <p>Currently, the file consists of 7 sheets that match the Beacon v2 Models.</p> <p>Please use the flag <code>--gv</code> should you want to validate the data in the sheet &lt;genomicVariations&gt;.</p> <p>NB: If you have multiple CSV files instead of a XLSX file you can use the included utility csv2xlsx that will join all CSVs into 1 XLSX.</p> <pre><code> $ ./csv2xlsx *csv -o out.xlsx\n</code></pre> <p>B) A set of JSON (array) files that follow the Beacon Friendly Format. The files MUST be uncompressed and named &lt;analyses.json&gt;, &lt;biosamples.json&gt;, etc.</p> </li> <li> <p>Beacon v2 Models (with JSON pointers dereferenced)</p> <p>You should have them at <code>deref_schemas</code> directory.</p> </li> </ul> <p>Examples:</p> <pre><code>  $ ./bff-validator -i file.xlsx\n\n  $ $path/bff-validator -i file.xlsx -o my_bff_outdir\n\n  $ $path/bff-validator -i my_bff_in_dir/*json -s deref_schemas -o my_bff_out_dir\n\n  $ $path/bff-validator -i file.xlsx --gv --schema-dir deref_schemas --out-dir my_bff_out_dir\n</code></pre> <p>## TIPS ON FILLING OUT THE EXCEL TEMPLATE</p> <pre><code> * Please, before filling in any field, check out the provided template for ../../CINECA_synthetic_cohort_EUROPE_UK1/*xlsx\n * The script accepts Unicode characters (encoded with utf-8)\n * Header fields: \n    - Dots ('.') represent objects: \n        Examples (values):\n          1 - foo\n          2 - NCIT:C20197\n          3 - true # booleans\n          4 - [\"foo\",\"bar\",\"baz\"] # arrays are also allowed\n    - Underscores ('_') represent arrays: \n        * Up to 1D (e.g., individuals-&gt;measures_assyCode.id) the values are comma separated\n           Examples (values):\n            1 - measures_assayCode.id\n                LOINC:35925-4,LOINC:3141-9,LOINC:8308-9\n               measures_assayCode.label\n                BMI,Weight,Height-standing\n\n        * Others - Values for array fields start with '[' and end with ']'\n           Examples (values): \n            1 - [\"foo\":{\"bar\": \"baz\"}}]\n            2 - [\"foo\",\"bar\",\"baz\"]\n</code></pre> <p>## COMMON ERRORS AND SOLUTIONS</p> <pre><code> * Error message: , or } expected while parsing object/hash, at character offset 574 (before \"]\")\n   Solution: Make sure you have the right amount of opening or closing keys/brackets.\n</code></pre> <p>NB: You can use the flag <code>--ignore-validation</code> and check the temporary files at <code>-o</code> directory.</p>"},{"location":"about/about/","title":"About","text":""},{"location":"about/about/#documentation-author","title":"Documentation author:","text":"<ul> <li>Manuel Rueda, PhD</li> </ul>"},{"location":"about/about/#acknowledgments","title":"Acknowledgments","text":"<ul> <li>Bio and Dev Teams @ CRG-EGA</li> <li>Dietmar Fernadez-Orth, PhD</li> <li>Sabela de la Torre</li> </ul>"},{"location":"about/citation/","title":"Citation","text":"<p>Citation</p> <p>Rueda, M, Ariosa R. \"Beacon v2 Reference Implementation: a toolkit to enable federated sharing of genomic and phenotypic data\". Bioinformatics, btac568, DOI.</p> <p>Acknowledgement (<code>beacon2-ri-tools</code>)</p> <p>This study was funded by ELIXIR, the research infrastructure for life-science data (ELIXIR Beacon Implementation Studies 2019-2021 and 2022-2023).</p>"},{"location":"download-and-installation/docker-based/","title":"Containerized Installation","text":""},{"location":"download-and-installation/docker-based/#downloading-required-databases-and-software","title":"Downloading Required Databases and Software","text":"<p>First, we need to download the necessary databases and software. In contrast to <code>beacon2-ri-tools</code>, where the data was bundled inside the container to provide a zero-configuration experience for users, we now store the data externally. This change improves data persistence and allows software updates without requiring a full re-download of all data.</p>"},{"location":"download-and-installation/docker-based/#step-1-download-required-files","title":"Step 1: Download Required Files","text":"<p>Navigate to a directory with at least 150GB of available space and run:</p> <pre><code>wget https://raw.githubusercontent.com/CNAG-Biomedical-Informatics/beacon2-cbi-tools/main/scripts/01_download_external_data.py\n</code></pre> <p>Then execute the script:</p> <pre><code>python3 01_download_external_data.py\n</code></pre> <p>Note: Google Drive can sometimes restrict downloads. If you encounter an error, use the provided error URL in a browser to retrieve the file manually.</p>"},{"location":"download-and-installation/docker-based/#step-2-verify-download-integrity","title":"Step 2: Verify Download Integrity","text":"<p>Run a checksum to ensure the files were not corrupted:</p> <pre><code>md5sum -c data.tar.gz.md5\n</code></pre>"},{"location":"download-and-installation/docker-based/#step-3-reassemble-split-files","title":"Step 3: Reassemble Split Files","text":"<p>The downloaded data is split into parts. Reassemble it into a single tar archive (~130GB required):</p> <pre><code>cat data.tar.gz.part-?? &gt; data.tar.gz\n</code></pre> <p>Once the files are successfully merged, delete the split parts to free up space:</p> <pre><code>rm data.tar.gz.part-??\n</code></pre>"},{"location":"download-and-installation/docker-based/#step-4-extract-data","title":"Step 4: Extract Data","text":"<p>Extract the tar archive:</p> <pre><code>tar -xzvf data.tar.gz\n</code></pre>"},{"location":"download-and-installation/docker-based/#step-5-configure-path-in-snpeff","title":"Step 5: Configure Path in SnpEff","text":"<p>In the downloaded data: </p> <p>Update the <code>data.dir</code> variable in SnpEff config file:</p> <pre><code>/path/to/downloaded/data/soft/NGSutils/snpEff_v5.0/snpEff.config\n</code></pre> <p>Ensure it points to the correct location of your downloaded data.</p>"},{"location":"download-and-installation/docker-based/#method-1-installing-from-docker-hub","title":"Method 1: Installing from Docker Hub","text":"<p>Pull the latest Docker image from Docker Hub:</p> <pre><code>docker pull manuelrueda/beacon2-cbi-tools:latest\ndocker image tag manuelrueda/beacon2-cbi-tools:latest cnag/beacon2-cbi-tools:latest\n</code></pre>"},{"location":"download-and-installation/docker-based/#method-2-installing-from-dockerfile","title":"Method 2: Installing from Dockerfile","text":"<p>Download the <code>Dockerfile</code> from GitHub:</p> <pre><code>wget https://raw.githubusercontent.com/CNAG-Biomedical-Informatics/beacon2-cbi-tools/main/docker/Dockerfile\n</code></pre> <p>Then build the container:</p> <ul> <li>For Docker version 19.03 and above (supports buildx):</li> </ul> <pre><code>docker buildx build -t cnag/beacon2-cbi-tools:latest .\n</code></pre> <ul> <li>For Docker versions older than 19.03 (no buildx support):</li> </ul> <pre><code>docker build -t cnag/beacon2-cbi-tools:latest .\n</code></pre>"},{"location":"download-and-installation/docker-based/#running-the-container","title":"Running the Container","text":"<pre><code># Please update '/media/mrueda/4TBB/beacon2-cbi-tools-data' with the location of your data. Do not touch the mounting point ':/beacon2-cbi-tools-data'\ndocker run -tid --volume /media/mrueda/4TBB/beacon2-cbi-tools-data:/beacon2-cbi-tools-data --name beacon2-cbi-tools cnag/beacon2-cbi-tools:latest\n\n# To check the containers\ndocker ps  # list your containers, beacon2-cbi-tools should be there\n\n# Connect to the container interactively\ndocker exec -ti beacon2-cbi-tools bash\n</code></pre> <p>Alternatively, you can run commands from the host, like this:</p> <p>First, create an alias to simplify invocation:</p> <pre><code>alias bff-tools='docker exec -ti beacon2-cbi-tools /usr/share/beacon2-cbi-tools/bin/bff-tools'\n</code></pre> <p>Then run:</p> <pre><code>bff-tools\n</code></pre>"},{"location":"download-and-installation/docker-based/#testing-the-deployment","title":"Testing the deployment","text":"<p>Go to directory <code>test</code> and execute:</p> <pre><code>bash 02_test_deployment.sh\n</code></pre>"},{"location":"download-and-installation/docker-based/#mongodb-installation-optional-only-for-loadfull-modes","title":"MongoDB Installation (Optional: Only for <code>load/full</code> modes)","text":"<p>If you don't already have MongoDB installed in a separate container, follow these steps.</p>"},{"location":"download-and-installation/docker-based/#step-1-download-docker-composeyml","title":"Step 1: Download <code>docker-compose.yml</code>","text":"<pre><code>wget https://raw.githubusercontent.com/CNAG-Biomedical-Informatics/beacon2-cbi-tools/main/docker/docker-compose.yml\n</code></pre>"},{"location":"download-and-installation/docker-based/#step-2-start-mongodb","title":"Step 2: Start MongoDB","text":"<pre><code>docker network create my-app-network\ndocker compose up -d\n</code></pre> <p>Mongo Express will be accessible at <code>http://localhost:8081</code> with default credentials <code>admin</code> and <code>pass</code>.</p> <p>IMPORTANT: If you plan to load data into MongoDB from inside the <code>beacon2-cbi-tools</code> container, read the section Access MongoDB from inside the container before proceeding.</p>"},{"location":"download-and-installation/docker-based/#access-mongodb-from-inside-the-container","title":"Access MongoDB from Inside the Container","text":"<p>If you want to load data from inside the <code>beacon2-cbi-tools</code> container directly into the <code>mongo</code> container, both containers must be on the same network.</p>"},{"location":"download-and-installation/docker-based/#option-a-before-running-the-container","title":"Option A: Before running the container","text":"<pre><code>docker run -tid --network=my-app-network --volume /media/mrueda/4TBB/beacon2-cbi-tools-data:/beacon2-cbi-tools-data --name beacon2-cbi-tools cnag/beacon2-cbi-tools:latest\n</code></pre>"},{"location":"download-and-installation/docker-based/#option-b-after-running-the-container","title":"Option B: After running the container","text":"<pre><code>docker network connect my-app-network beacon2-cbi-tools\n</code></pre>"},{"location":"download-and-installation/docker-based/#system-requirements","title":"System requirements","text":"<ul> <li>OS/ARCH supported: linux/amd64 and linux/arm64).</li> <li>Ideally a Debian-based distribution (Ubuntu or Mint), but any other (e.g., CentOS, OpenSUSE) should do as well (untested).</li> <li>Docker and docker compose</li> <li>Perl 5 (&gt;= 5.10 core; installed by default in most Linux distributions). Check the version with perl -v</li> <li>4GB of RAM (ideally 16GB).</li> <li>&gt;= 1 core (ideally i7 or Xeon).</li> <li>At least 200GB HDD.</li> </ul> <p>Perl itself does not require much RAM (max load ~400MB), but external tools (e.g., <code>mongod</code> [MongoDB's daemon]) do.</p>"},{"location":"download-and-installation/docker-based/#common-errors-symptoms-and-treatment","title":"Common errors: Symptoms and treatment","text":"<ul> <li>Dockerfile:<pre><code>  * DNS errors\n\n    - Error: Temporary failure resolving 'foo'\n\n      Solution: https://askubuntu.com/questions/91543/apt-get-update-fails-to-fetch-files-temporary-failure-resolving-error\n</code></pre> </li> </ul>"},{"location":"download-and-installation/docker-based/#references","title":"References","text":"<ol> <li> <p>BCFtools    Danecek P, Bonfield JK, et al. Twelve years of SAMtools and BCFtools. Gigascience (2021) 10(2):giab008. Read more.</p> </li> <li> <p>SnpEff    Cingolani P, Platts A, Wang le L, Coon M, et al. Fly (Austin). 2012 Apr-Jun;6(2):80-92. PMID: 22728672.</p> </li> <li> <p>SnpSift    Cingolani, P., et. al. Frontiers in Genetics, 3, 2012.</p> </li> <li> <p>dbNSFP v4 </p> </li> <li>Liu X, Jian X, and Boerwinkle E. Human Mutation. 32:894-899.</li> <li>Liu X, Wu C, Li C, and Boerwinkle E. Human Mutation. 37:235-241.</li> <li>Liu X, Li C, Mou C, Dong Y, and Tu Y. Genome Medicine. 12:103.</li> </ol>"},{"location":"download-and-installation/non-containerized/","title":"Non-containerized installation","text":""},{"location":"download-and-installation/non-containerized/#downloading-required-databases-and-software","title":"Downloading Required Databases and Software","text":"<p>First, we need to download the necessary databases and software.</p>"},{"location":"download-and-installation/non-containerized/#step-1-download-required-files","title":"Step 1: Download Required Files","text":"<p>Navigate to a directory with at least 150GB of available space and run:</p> <pre><code>wget https://raw.githubusercontent.com/CNAG-Biomedical-Informatics/beacon2-cbi-tools/main/scripts/01_download_external_data.py\n</code></pre> <p>Then execute the script:</p> <pre><code>python3 01_download_external_data.py\n</code></pre> <p>Note: Google Drive can sometimes restrict downloads. If you encounter an error, use the provided error URL in a browser to retrieve the file manually.</p>"},{"location":"download-and-installation/non-containerized/#step-2-verify-download-integrity","title":"Step 2: Verify Download Integrity","text":"<p>Run a checksum to ensure the files were not corrupted:</p> <pre><code>md5sum -c data.tar.gz.md5\n</code></pre>"},{"location":"download-and-installation/non-containerized/#step-3-reassemble-split-files","title":"Step 3: Reassemble Split Files","text":"<p>The downloaded data is split into parts. Reassemble it into a single tar archive (~130GB required):</p> <pre><code>cat data.tar.gz.part-?? &gt; data.tar.gz\n</code></pre> <p>Once the files are successfully merged, delete the split parts to free up space:</p> <pre><code>rm data.tar.gz.part-??\n</code></pre>"},{"location":"download-and-installation/non-containerized/#step-4-extract-data","title":"Step 4: Extract Data","text":"<p>Extract the tar archive:</p> <pre><code>tar -xzvf data.tar.gz\n</code></pre>"},{"location":"download-and-installation/non-containerized/#download-from-github","title":"Download from GitHub","text":"<p>First, we need to install a few system components:</p> <pre><code>sudo apt install libbz2-dev zlib1g-dev libncurses5-dev libncursesw5-dev liblzma-dev libcurl4-openssl-dev libssl-dev cpanminus python3-pip perl-doc default-jre\n</code></pre> <p>Let's install <code>mongosh</code> (only if you plan to load data into MongoDB)</p> <pre><code>wget -qO - https://www.mongodb.org/static/pgp/server-6.0.asc | sudo gpg --dearmor -o /usr/share/keyrings/mongodb-server-6.0.gpg\necho \"deb [signed-by=/usr/share/keyrings/mongodb-server-6.0.gpg arch=amd64,arm64] https://repo.mongodb.org/apt/ubuntu focal/mongodb-org/6.0 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-6.0.list\nsudo apt-get update\nsudo apt-get install -y mongodb-mongosh\n</code></pre> <p>Use <code>git clone</code> to get the latest (stable) version:</p> <pre><code>git clone https://github.com/CNAG-Biomedical-Informatics/beacon2-cbi-tools.git\ncd beacon2-cbi-tools\n</code></pre> <p>If you only new to update to the lastest version do:</p> <pre><code>git pull\n</code></pre> <p><code>bff-tools</code> is a Perl script (no compilation required) designed to run on the Linux command line. Internally, it acts as a wrapper that submits multiple pipelines through customizable Bash scripts (see an example here). While Perl and Bash are pre-installed on most Linux systems, a few additional dependencies must be installed separately.</p> <p>We use <code>cpanm</code> to install the CPAN modules. We'll install the dependencies at <code>~/perl5</code>:</p> <pre><code>cpanm --local-lib=~/perl5 local::lib &amp;&amp; eval $(perl -I ~/perl5/lib/perl5/ -Mlocal::lib)\ncpanm --notest --installdeps .\n</code></pre> <p>To ensure Perl recognizes your local modules every time you start a new terminal, run:</p> <pre><code>echo 'eval $(perl -I ~/perl5/lib/perl5/ -Mlocal::lib)' &gt;&gt; ~/.bashrc\n</code></pre> <p>We'll also need a few Python 3 modules:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"download-and-installation/non-containerized/#system-requirements","title":"System requirements","text":"<ul> <li>OS/ARCH supported: linux/amd64 and linux/arm64).</li> <li>Ideally a Debian-based distribution (Ubuntu or Mint), but any other (e.g., CentOS, OpenSUSE) should do as well (untested).</li> <li>Perl 5 (&gt;= 5.10 core; installed by default in most Linux distributions). Check the version with <code>perl -v</code></li> <li>4GB of RAM (ideally 16GB).</li> <li>&gt;= 1 core (ideally i7 or Xeon).</li> <li>At least 200GB HDD.</li> </ul> <p>The Perl itself does not need a lot of RAM (max load will reach 400MB), but external tools do (e.g., process <code>mongod</code> [MongoDB's daemon]).</p>"},{"location":"download-and-installation/non-containerized/#testing-the-deployment","title":"Testing the deployment","text":"<p>You may wanna install <code>jq</code> for running tests.</p> <p>Go to directory <code>test</code> and execute:</p> <pre><code>bash 02_test_deployment.sh\n</code></pre>"},{"location":"download-and-installation/non-containerized/#common-errors-symptoms-and-treatment","title":"Common errors: Symptoms and treatment","text":"<ul> <li> <p>Perl errors:</p> <ul> <li>Error: Unknown PerlIO layer \"gzip\" at (eval 10) line XXX</li> </ul> <p>Solution: </p> <p><code>cpanm PerlIO::gzip</code></p> <pre><code> ... or ...\n</code></pre> <p><code>sudo apt install libperlio-gzip-perl</code></p> </li> </ul>"},{"location":"download-and-installation/non-containerized/#references","title":"References","text":"<ol> <li> <p>BCFtools     Danecek P, Bonfield JK, et al. Twelve years of SAMtools and BCFtools. Gigascience (2021) 10(2):giab008 link</p> </li> <li> <p>SnpEff     \"A program for annotating and predicting the effects of single nucleotide polymorphisms, SnpEff: SNPs in the genome of Drosophila melanogaster strain w1118; iso-2; iso-3.\", Cingolani P, Platts A, Wang le L, Coon M, Nguyen T, Wang L, Land SJ, Lu X, Ruden DM. Fly (Austin). 2012 Apr-Jun;6(2):80-92. PMID: 22728672.</p> </li> <li> <p>SnpSift     \"Using Drosophila melanogaster as a model for genotoxic chemical mutational studies with a new program, SnpSift\", Cingolani, P., et. al., Frontiers in Genetics, 3, 2012.</p> </li> <li> <p>dbNSFP v4</p> <ol> <li>Liu X, Jian X, and Boerwinkle E. 2011. dbNSFP: a lightweight database of human non-synonymous SNPs and their functional predictions. Human Mutation. 32:894-899.</li> <li>Liu X, Jian X, and Boerwinkle E. 2013. dbNSFP v2.0: A Database of Human Non-synonymous SNVs and Their Functional Predictions and Annotations. Human Mutation. 34:E2393-E2402.</li> <li>Liu X, Wu C, Li C, and Boerwinkle E. 2016. dbNSFP v3.0: A One-Stop Database of Functional Predictions and Annotations for Human Non-synonymous and Splice Site SNVs. Human Mutation. 37:235-241.</li> <li>Liu X, Li C, Mou C, Dong Y, and Tu Y. 2020. dbNSFP v4: a comprehensive database of transcript-specific functional predictions and annotations for human nonsynonymous and splice-site SNVs. Genome Medicine. 12:103.</li> </ol> </li> </ol>"},{"location":"examples/hg38/","title":"hs37 (1000 Genomes Project version of GRCh37)","text":"<p>See the test directory.</p>"},{"location":"examples/hg38/#hg38-grch38","title":"hg38 (GRCh38)","text":""},{"location":"examples/hg38/#data-download","title":"Data Download","text":"<p>We download the data using <code>wget</code> since we could not use <code>tabix</code> directly:</p> <pre><code>wget ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/release/20190312_biallelic_SNV_and_INDEL/ALL.chr22.shapeit2_integrated_snvindels_v2a_27022019.GRCh38.phased.vcf.gz\n</code></pre> <p>Now, we index the VCF with <code>tabix</code>:</p> <pre><code>tabix -p vcf ALL.chr22.shapeit2_integrated_snvindels_v2a_27022019.GRCh38.phased.vcf.gz\n</code></pre> <p>Note: If your version of <code>tabix</code> accepts using <code>ftp</code> protocol:</p> <pre><code>#tabix -h ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/release/20190312_biallelic_SNV_and_INDEL/ALL.chr22.shapeit2_integrated_snvindels_v2a_27022019.GRCh38.phased.vcf.gz 22:10516173-11016173  | sed 's/^22    /chr22  /' | bgzip &gt; test_1000G_hg38.vcf.gz\n</code></pre>"},{"location":"examples/hg38/#data-subset","title":"Data subset","text":"<p>Next, we need to convert the GRCh38 file to hg38. This involves adding the prefix 'chr' to '22' to obtain <code>chr22</code>. </p> <pre><code>tabix -h ALL.chr22.shapeit2_integrated_snvindels_v2a_27022019.GRCh38.phased.vcf.gz 22:10516173-11016173 | sed -e 's/##contig=&lt;ID=22&gt;/##contig=&lt;ID=chr22&gt;/' -e 's/^22\\t/chr22\\t/' | bgzip &gt; test_1000G_hg38.vcf.gz\n</code></pre>"},{"location":"examples/hg38/#run-bff-tools","title":"Run <code>bff-tools</code>","text":"<p>The simplest task is to convert a <code>VCF</code> file to the <code>BFF</code> format. The resulting files will be located in the <code>beacon_*/vcf/</code> directory.</p> <pre><code>../bin/bff-tools vcf -i test_1000G_hg38.vcf.gz -p param_hg38.yaml\n# Here we're using 'hg38' as the reference genome.\n</code></pre>"},{"location":"examples/hg38/#alternative-bff-tools-modes","title":"Alternative <code>bff-tools</code> modes","text":"<p>If your <code>mongo</code> container is set up and running, you can convert the <code>VCF</code> and load the data into MongoDB in a single step using the <code>full</code> mode:</p> <pre><code>../bin/bff-tools full -i test_1000G_hg38.vcf.gz -p param_hg38.yaml\n# This runs both 'vcf' and 'load' steps together.\n</code></pre> <p>The result of the MongoDB import will be located in the <code>beacon_*/mongodb/</code> directory.</p>"},{"location":"examples/hg38/#loading-other-beacon-v2-model-entities","title":"Loading other Beacon v2 Model entities","text":"<p>To import other Beacon v2 Model entities into MongoDB (without converting VCFs), use the <code>load</code> mode with a YAML file:</p> <pre><code>../bin/bff-tools load -p load.yaml\n</code></pre>"},{"location":"help/faq/","title":"Frequently Asked Questions","text":"Are Beacon v2 <code>genomicVariations.variation.location.interval.{start,end}</code> coordinates 0-based or 1-based? <p>They are 0-based.</p> I have an error when attempting to use <code>beacon vcf</code>, what should I do? <ul> <li> <p>In 9 out of 10 cases, the error comes from BCFtools and is related to the reference genome specified in the parameters file. The options are typically hg19, hg38 (which use <code>chr</code> prefixes), and hs37 (which do not). Ensure that your VCF\u2019s contigs match the FASTA file or modify your <code>config.yaml</code> accordingly.</p> </li> <li> <p>Additionally, BCFtools may complain about the number of fields (for example, in the INFO field). In such cases, you can try fixing the VCF manually or use:</p> </li> </ul> <pre><code>bcftools annotate -x INFO/IDREP input.vcf.gz | gzip &gt; output.vcf.gz\n</code></pre> Can I use SINGLE-SAMPLE and MULTI-SAMPLE VCFs? <p>Yes, you can use both. MongoDB allows for incremental loads so single-sample VCFs are acceptable (you don\u2019t need to merge them into a multisample VCF). The connection between samples and variants is maintained in the <code>datasets</code> collection (or <code>cohorts</code>).</p> Can I use genomic VCF (gVCF)? <p>Yes, but first you will need to convert them to a standard VCF. For example, you can use:</p> <pre><code>bcftools convert --gvcf2vcf --fasta ref.fa input.g.vcf\n</code></pre> <p>We are interested only in positions with ALT alleles. A \u201cquick and dirty\u201d solution with common Linux tools is:</p> <pre><code>zcat input.g.vcf.gz | awk '$5 != \"&lt;NON_REF&gt;\"' | sed 's#,&lt;NON_REF&gt;##' | gzip &gt; output.vcf.gz\n</code></pre> In <code>bff-tools vcf</code> mode, why are we re-annotating VCFs | Can I use my own annotations? <p>The goal of re-annotation is to ensure consistency across the community. To create the <code>genomicVariationsVcf.json.gz</code> BFF, we parse an annotated VCF\u2014this guarantees that the essential fields are present. Any previous annotations will be discarded. This approach has been instrumental in over 1,000 deployments for testing Beacon v2 API implementations.</p> <p>That said, if you know what you're doing and your <code>VCF</code> already contains the essential <code>ANN</code> fields, you can disable annotation by setting:</p> <pre><code>annotate: false\n</code></pre> <p>in the parameters file. Do it at your own risk </p> <p>If you have internal annotations of value, you can add alternative genomic variations by completing the corresponding tab in the provided XLSX. The resulting file (<code>genomicVariations.json</code>), together with <code>genomicVariationsVcf.json.gz</code>, will be loaded into the MongoDB collection genomicVariations. See this tutorial for more details.</p> Is there an alternative to the Excel file for generating metadata/phenotypic data? <p>Yes. You can use CSV or JSON files directly as input for the <code>bff-tools validate</code> (a.k.a., <code>bff-validator</code>) utility. For detailed instructions, refer to the bff-validator manual.</p> <p>Alternatively, if your clinical data is in REDCap, OMOP CDM, Phenopackets v2, or raw CSV format, consider using the Convert-Pheno tool.</p> <code>bff-tool validate</code> (a.k.a., <code>bff-validator)</code> specification mismatches <p>By default, <code>bff-validator</code> validates your data against the schemas bundled with your <code>beacon2-cbi-tools</code> version. If you encounter warnings (e.g., objects matching multiple possibilities in <code>oneOf</code> keywords), simply use the flag <code>--ignore-validation</code> when generating your <code>.json</code> files.</p> Do you load all variations present in a VCF file? <p>Yes, we do not apply filters (e.g., based on <code>FILTER</code> or <code>QUAL</code> fields) when loading variations, although we store those values in case they are needed later.</p> Do you have any recommendations on how to speed up the data ingestion process? <p>Metadata/phenoclinic data ingestion is typically fast (processing thousands to tens of thousands of values in seconds or minutes). However, VCF processing (especially for WGS data with &gt;100M variants) can be slower. Consider the following:</p> <ol> <li>Split your VCF by chromosome:</li> <li>Using community tools:      <pre><code>bcftools view input.vcf.gz --regions chr1\n</code></pre></li> <li>Alternatively:      <pre><code>tabix -p vcf input.vcf.gz\ntabix input.vcf.gz chr1 | bgzip &gt; chr1.vcf.gz\n</code></pre></li> <li> <p>Or with Linux tools:      <pre><code>zcat input.vcf.gz | awk '/^#/ || $1==\"chr1\"' | bgzip &gt; chr1.vcf.gz\n</code></pre></p> </li> <li> <p>Use parallel processing to submit jobs.</p> </li> </ol> Can I use parallel jobs to perform data ingestion into MongoDB? <p>Yes, you can use parallel jobs; however, note that it may slightly slow down the ingestion process.</p> When performing incremental uploads, do I need to re-index MongoDB? <p>No. Indexes are created during the first data load and are updated automatically with each insert operation. Subsequent re-indexing attempts are discarded (the operation is idempotent).</p> Where do I get full WGS VCF for the CINECA synthetic cohort EUROPE UK1? <p>For full WGS data (\u224820\u202fGB for 2,504 synthetic individuals), request access and download from the EGA. See this document for details.</p> Are <code>beacon2-cbi-tools</code> free? <p>Yes, it is free and open source. The data ingestion tools are released under the GNU General Public License v3.0, and the included CINECA_synthetic_cohort_EUROPE_UK1 dataset is under a CC-BY license.</p> Should I update to the <code>latest</code> version? <p>Yes. We recommend checking our GitHub repositories (beacon2-cbi-tools for the latest updates.</p> How do I cite <code>beacon2-cbi-tools</code>? <p>You can cite the Beacon v2 Reference Implementation paper. Thx!</p> <p>Citation</p> <p>Rueda, M, Ariosa R. \"Beacon v2 Reference Implementation: a toolkit to enable federated sharing of genomic and phenotypic data\". Bioinformatics, btac568, DOI.</p>"},{"location":"help/faq/#last-change-2025-03-23-by-manuel-rueda","title":"last change 2025-03-23 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-03-23-by-manuel-rueda_1","title":"last change 2025-03-23 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-03-23-by-manuel-rueda_2","title":"last change 2025-03-23 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-03-23-by-manuel-rueda_3","title":"last change 2025-03-23 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-03-23-by-manuel-rueda_4","title":"last change 2025-03-23 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-03-23-by-manuel-rueda_5","title":"last change 2025-03-23 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-03-23-by-manuel-rueda_6","title":"last change 2025-03-23 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-03-23-by-manuel-rueda_7","title":"last change 2025-03-23 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-03-23-by-manuel-rueda_8","title":"last change 2025-03-23 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-03-23-by-manuel-rueda_9","title":"last change 2025-03-23 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-03-23-by-manuel-rueda_10","title":"last change 2025-03-23 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-03-23-by-manuel-rueda_11","title":"last change 2025-03-23 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-03-23-by-manuel-rueda_12","title":"last change 2025-03-23 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-03-23-by-manuel-rueda_13","title":"last change 2025-03-23 by Manuel Rueda","text":""},{"location":"help/faq/#last-change-2025-03-23-by-manuel-rueda_14","title":"last change 2025-03-23 by Manuel Rueda","text":""},{"location":"help/troubleshooting/","title":"Troubleshooting","text":"<p>This section provides guidance for resolving common issues.</p>"},{"location":"help/troubleshooting/#common-issues","title":"Common Issues","text":"<ul> <li>Issue 1: Description and solution.</li> <li>Issue 2: Description and solution.</li> </ul>"}]}